INFO - main - Starting training iteration 0
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604062)[0m   action_dist_inputs = np.log(mcts_policies)
2025-04-15 10:31:21,821	WARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!
2025-04-15 10:31:22,553	WARNING replay_buffer.py:61 -- Estimated max memory usage for replay buffer is 37.42486528 GB (2048.0 batches of size 64, 18273860 bytes each), available system memory is 67.18121984 GB
2025-04-15 10:31:23,692	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!
/home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  F.linear(
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return F.linear(input, self.weight, self.bias)
2025-04-15 10:31:24,090	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/autograd/graph.py:823: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2m[36m(pid=3607858)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=3604057)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 11x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 11x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 7x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
INFO - main - Starting training iteration 1
INFO - main - Starting training iteration 2
INFO - main - Starting training iteration 3
INFO - main - Starting training iteration 4
[2m[36m(RolloutWorker pid=3607858)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604061)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604061)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604061)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604061)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[33m(raylet)[0m [2025-04-15 10:32:17,209 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3613534)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3604057)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m   F.linear([32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3607858)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
2025-04-15 10:40:34,963	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 10:40:34,987	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/alphazero_assistant/infinite_blocks_true/human_pikl/2025-04-15_10-23-15/1/checkpoint_000005
INFO - main - Starting training iteration 5
INFO - main - Starting training iteration 6
INFO - main - Starting training iteration 7
INFO - main - Starting training iteration 8
[2m[36m(RolloutWorker pid=3613534)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604061)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604061)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604062)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[33m(raylet)[0m [2025-04-15 10:41:17,213 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3618426)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3604057)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604057)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m   F.linear([32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3613534)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
2025-04-15 10:49:08,765	ERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.
[2m[33m(raylet)[0m [2025-04-15 10:49:17,217 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 10:51:17,342 E 3602856 3602856] (raylet) node_manager.cc:3007: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
INFO - main - Starting training iteration 9
[2m[36m(pid=3621673)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3618426)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3618426)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=3621688)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2025-04-15 10:51:36,313	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 10:51:36,324	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 10:51:36,760	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 10:51:36,786	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/alphazero_assistant/infinite_blocks_true/human_pikl/2025-04-15_10-23-15/1/checkpoint_000010
INFO - main - Starting training iteration 10
INFO - main - Starting training iteration 11
INFO - main - Starting training iteration 12
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3621688)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3621688)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604062)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[36m(pid=3624254)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3621688)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3621688)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3621688)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604061)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604061)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3621688)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3621688)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-15 10:52:17,342 E 3602856 3602856] (raylet) node_manager.cc:3007: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 10:53:52,586	ERROR actor_manager.py:500 -- Ray error, taking actor 3 out of service. The actor died unexpectedly before finishing this task.
[2m[33m(raylet)[0m [2025-04-15 11:03:17,362 E 3602856 3602856] (raylet) node_manager.cc:3007: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3627639)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3624254)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3624254)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=3627651)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2025-04-15 11:03:27,623	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 11:03:27,635	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Starting training iteration 13
INFO - main - Starting training iteration 14
2025-04-15 11:03:52,804	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 11:03:52,832	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/alphazero_assistant/infinite_blocks_true/human_pikl/2025-04-15_10-23-15/1/checkpoint_000015
INFO - main - Starting training iteration 15
INFO - main - Starting training iteration 16
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3627639)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3627639)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604062)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[36m(pid=3631158)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3627651)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3627651)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3627651)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3627651)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3627651)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-15 11:05:17,363 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 11:05:58,506	ERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.
[2m[33m(raylet)[0m [2025-04-15 11:15:17,369 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3634805)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3631158)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3631158)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=3634813)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2025-04-15 11:15:33,236	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 11:15:33,247	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Starting training iteration 17
INFO - main - Starting training iteration 18
INFO - main - Starting training iteration 19
2025-04-15 11:16:10,849	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 11:16:10,874	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/MbagAlphaZero/2_players/11x10x10/craftassist/alphazero_assistant/infinite_blocks_true/human_pikl/2025-04-15_10-23-15/1/checkpoint_000020
INFO - main - Starting training iteration 20
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3634813)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3634813)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604061)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604061)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[33m(raylet)[0m [2025-04-15 11:16:17,369 E 3602856 3602856] (raylet) node_manager.cc:3007: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3638168)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3634813)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3634813)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3634813)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3604061)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3604061)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3634813)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3634813)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-15 11:17:17,376 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 11:18:01,606	ERROR actor_manager.py:500 -- Ray error, taking actor 3 out of service. The actor died unexpectedly before finishing this task.
[2m[36m(pid=3641329)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3638168)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3638168)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=3641341)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[33m(raylet)[0m [2025-04-15 11:27:17,381 E 3602856 3602856] (raylet) node_manager.cc:3007: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 11:27:18,719	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
2025-04-15 11:27:18,731	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'mbag.rllib.alpha_zero.alpha_zero_policy.MbagAlphaZeroPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Starting training iteration 21
INFO - main - Starting training iteration 22
INFO - main - Starting training iteration 23
INFO - main - Starting training iteration 24
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   F.linear(
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=3604062)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 5x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 5x across cluster][0m
[2m[36m(RolloutWorker pid=3604062)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=3604062)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[36m(pid=3644392)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3641341)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3641341)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-15 11:28:17,388 E 3602856 3602856] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3644748)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3644392)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3644392)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=3644941)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3644984)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3644984)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=3645220)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3645263)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3645263)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(pid=3645496)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3645539)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 4x across cluster][0m
[2m[36m(RolloutWorker pid=3645539)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 4x across cluster][0m
[2m[36m(pid=3645775)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3645837)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3645837)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=3646051)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646148)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646148)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=3646327)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646425)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646425)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=3646604)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646797)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3646797)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=3646883)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3647076)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3647076)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-15 11:29:17,388 E 3602856 3602856] (raylet) node_manager.cc:3007: 51 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 11:29:54,340	ERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.
[2m[33m(raylet)[0m [2025-04-15 11:30:17,389 E 3602856 3602856] (raylet) node_manager.cc:3007: 110 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:31:17,628 E 3602856 3602856] (raylet) node_manager.cc:3007: 110 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:32:17,629 E 3602856 3602856] (raylet) node_manager.cc:3007: 109 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:33:17,629 E 3602856 3602856] (raylet) node_manager.cc:3007: 109 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:34:17,630 E 3602856 3602856] (raylet) node_manager.cc:3007: 109 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:35:17,630 E 3602856 3602856] (raylet) node_manager.cc:3007: 109 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:36:17,683 E 3602856 3602856] (raylet) node_manager.cc:3007: 109 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[33m(raylet)[0m [2025-04-15 11:37:17,684 E 3602856 3602856] (raylet) node_manager.cc:3007: 108 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3683209)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=3647119)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3647119)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3683209)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3683209)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3683209)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3683209)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-15 11:38:17,684 E 3602856 3602856] (raylet) node_manager.cc:3007: 27 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-15 11:39:01,253	ERROR actor_manager.py:500 -- Ray error, taking actor 3 out of service. The actor died unexpectedly before finishing this task.
[2m[33m(raylet)[0m [2025-04-15 11:39:17,685 E 3602856 3602856] (raylet) node_manager.cc:3007: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR - train_mbag - Failed after 1:16:09!
Traceback (most recent calls WITHOUT Sacred internals):
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 1052, in main
    result = trainer.train()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/tune/trainable/trainable.py", line 455, in train
    self.log_result(result)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 2163, in log_result
    self.callbacks.on_train_result(algorithm=self, result=result)
  File "/home/elle/Documents/AssistanceZero/mbag/rllib/callbacks.py", line 41, in on_train_result
    algorithm.workers.foreach_worker(update_worker_envs_global_timestep)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 680, in foreach_worker
    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 76, in handle_remote_call_result_errors
    raise r.get()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 481, in __fetch_result
    result = ray.get(r)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/worker.py", line 2549, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.51.114.46, ID: 0aa38c2f33f5dcb89a97387ea3a572833a03f495b7dfcec4fe26e64f) where the task (task ID: ffffffffffffffff231906f34e3d29fda612930901000000, name=RolloutWorker.__init__, pid=3683175, memory used=0.21GB) was running was 60.42GB / 62.57GB (0.965715), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7f7aa38dc6f9641f5b87034103fec2d294abb39608df2c8e5d69d751) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.51.114.46`. To see the logs of the worker, use `ray logs worker-7f7aa38dc6f9641f5b87034103fec2d294abb39608df2c8e5d69d751*out -ip 10.51.114.46. Top 10 memory users:
PID	MEM(GB)	COMMAND
3602657	16.52	python -m mbag.scripts.train with assistancezero_assistant checkpoint_to_load_policies=data/logs/Mba...
3604061	10.62	ray::RolloutWorker.apply
3604062	10.53	ray::RolloutWorker.apply
3641329	10.52	ray::RolloutWorker.apply
3591089	0.62	/snap/code/190/usr/share/code/code /home/elle/.vscode/extensions/ms-python.vscode-pylance-2025.4.1/d...
2784	0.54	/snap/firefox/3836/usr/lib/firefox/firefox
3590798	0.44	/snap/code/190/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=e...
3044732	0.42	/snap/firefox/3836/usr/lib/firefox/firefox -contentproc -childID 334 -isForBrowser -prefsLen 28700 -...
792769	0.39	/snap/firefox/3836/usr/lib/firefox/firefox -contentproc -childID 164 -isForBrowser -prefsLen 28700 -...
3572993	0.38	/usr/bin/gnome-shell
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[0m
