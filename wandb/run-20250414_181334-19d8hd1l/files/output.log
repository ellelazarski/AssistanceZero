INFO - main - Starting training iteration 0
[2m[36m(pid=3108009)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=3108010)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3108010)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3108010)[0m 2025-04-14 18:12:49,914	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.[32m [repeated 19x across cluster][0m
[2m[36m(RolloutWorker pid=3108013)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 40x across cluster][0m
[2m[36m(RolloutWorker pid=3108013)[0m   F.linear([32m [repeated 20x across cluster][0m
[2m[36m(RolloutWorker pid=3108010)[0m no player locations found in observation[32m [repeated 59x across cluster][0m
[2m[36m(RolloutWorker pid=3108010)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 19x across cluster][0m
2025-04-14 18:13:48,326	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!
2025-04-14 18:13:49,425	WARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!
/home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  F.linear(
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return F.linear(input, self.weight, self.bias)
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/autograd/graph.py:823: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2m[36m(pid=3111566)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3108020)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3108020)[0m   F.linear([32m [repeated 19x across cluster][0m
[2m[36m(RolloutWorker pid=3108020)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 20x across cluster][0m
[2m[36m(RolloutWorker pid=3111566)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3111566)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3111566)[0m 2025-04-14 18:13:50,538	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3111566)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3111566)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3111566)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3111566)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3111566)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:14:44,306 E 3106800 3106800] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 203ada9541ea52ab7182b38c4f614a4a4e2af4cc9259b8c9b82e1b7b, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3112156)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3111566)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3111566)[0m   F.linear(
[2m[36m(RolloutWorker pid=3111566)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3112156)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3112156)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3112156)[0m 2025-04-14 18:15:00,399	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3112156)[0m   F.linear(
[2m[36m(RolloutWorker pid=3112156)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112156)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3112156)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112156)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112156)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3112156)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:15:44,307 E 3106800 3106800] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 203ada9541ea52ab7182b38c4f614a4a4e2af4cc9259b8c9b82e1b7b, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3112739)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3112156)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3112739)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3112739)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3112739)[0m 2025-04-14 18:16:10,790	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3112739)[0m   F.linear(
[2m[36m(RolloutWorker pid=3112739)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112739)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3112739)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112739)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3112739)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3112739)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:16:44,307 E 3106800 3106800] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 203ada9541ea52ab7182b38c4f614a4a4e2af4cc9259b8c9b82e1b7b, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
Iteration 0 result: {'custom_metrics': {'human/num_noop_mean': 8.764705882352942, 'human/num_noop_min': 0, 'human/num_noop_max': 20, 'human/num_place_block_mean': 0.5294117647058824, 'human/num_place_block_min': 0, 'human/num_place_block_max': 3, 'human/num_correct_place_block_mean': 0.08823529411764706, 'human/num_correct_place_block_min': 0, 'human/num_correct_place_block_max': 1, 'human/num_break_block_mean': 0.058823529411764705, 'human/num_break_block_min': 0, 'human/num_break_block_max': 1, 'human/num_correct_break_block_mean': 0.058823529411764705, 'human/num_correct_break_block_min': 0, 'human/num_correct_break_block_max': 1, 'human/num_move_pos_x_mean': 6.9411764705882355, 'human/num_move_pos_x_min': 0, 'human/num_move_pos_x_max': 14, 'human/num_move_neg_x_mean': 6.470588235294118, 'human/num_move_neg_x_min': 0, 'human/num_move_neg_x_max': 16, 'human/num_move_pos_y_mean': 6.588235294117647, 'human/num_move_pos_y_min': 0, 'human/num_move_pos_y_max': 14, 'human/num_move_neg_y_mean': 5.852941176470588, 'human/num_move_neg_y_min': 0, 'human/num_move_neg_y_max': 16, 'human/num_move_pos_z_mean': 5.617647058823529, 'human/num_move_pos_z_min': 1, 'human/num_move_pos_z_max': 16, 'human/num_move_neg_z_mean': 6.676470588235294, 'human/num_move_neg_z_min': 0, 'human/num_move_neg_z_max': 21, 'human/num_unintentional_noop_mean': 0.0, 'human/num_unintentional_noop_min': 0, 'human/num_unintentional_noop_max': 0, 'human/own_reward_mean': -0.29411764705882354, 'human/own_reward_min': -2.0, 'human/own_reward_max': 1.0, 'human/goal_dependent_reward_mean': -0.29411764705882354, 'human/goal_dependent_reward_min': -2.0, 'human/goal_dependent_reward_max': 1.0, 'human/goal_independent_reward_mean': 0.0, 'human/goal_independent_reward_min': 0.0, 'human/goal_independent_reward_max': 0.0, 'goal_similarity_mean': 882.5, 'goal_similarity_min': 665.0, 'goal_similarity_max': 1012.0, 'goal_percentage_mean': -0.001349012587022544, 'goal_percentage_min': -0.007936507936507936, 'goal_percentage_max': 0.0030864197530864196, 'goal_distance_mean': 217.5, 'goal_distance_min': 88.0, 'goal_distance_max': 435.0, 'goal_percentage_1_min_mean': -0.001349012587022544, 'goal_percentage_1_min_min': -0.007936507936507936, 'goal_percentage_1_min_max': 0.0030864197530864196, 'human/per_minute_metrics/num_noop_1_min_mean': 8.529411764705882, 'human/per_minute_metrics/num_noop_1_min_min': 0, 'human/per_minute_metrics/num_noop_1_min_max': 20, 'human/per_minute_metrics/num_place_block_1_min_mean': 0.5294117647058824, 'human/per_minute_metrics/num_place_block_1_min_min': 0, 'human/per_minute_metrics/num_place_block_1_min_max': 3, 'human/per_minute_metrics/num_correct_place_block_1_min_mean': 0.08823529411764706, 'human/per_minute_metrics/num_correct_place_block_1_min_min': 0, 'human/per_minute_metrics/num_correct_place_block_1_min_max': 1, 'human/per_minute_metrics/num_break_block_1_min_mean': 0.058823529411764705, 'human/per_minute_metrics/num_break_block_1_min_min': 0, 'human/per_minute_metrics/num_break_block_1_min_max': 1, 'human/per_minute_metrics/num_correct_break_block_1_min_mean': 0.058823529411764705, 'human/per_minute_metrics/num_correct_break_block_1_min_min': 0, 'human/per_minute_metrics/num_correct_break_block_1_min_max': 1, 'human/per_minute_metrics/num_move_pos_x_1_min_mean': 6.823529411764706, 'human/per_minute_metrics/num_move_pos_x_1_min_min': 0, 'human/per_minute_metrics/num_move_pos_x_1_min_max': 14, 'human/per_minute_metrics/num_move_neg_x_1_min_mean': 6.235294117647059, 'human/per_minute_metrics/num_move_neg_x_1_min_min': 0, 'human/per_minute_metrics/num_move_neg_x_1_min_max': 13, 'human/per_minute_metrics/num_move_pos_y_1_min_mean': 6.4411764705882355, 'human/per_minute_metrics/num_move_pos_y_1_min_min': 0, 'human/per_minute_metrics/num_move_pos_y_1_min_max': 14, 'human/per_minute_metrics/num_move_neg_y_1_min_mean': 5.411764705882353, 'human/per_minute_metrics/num_move_neg_y_1_min_min': 0, 'human/per_minute_metrics/num_move_neg_y_1_min_max': 11, 'human/per_minute_metrics/num_move_pos_z_1_min_mean': 5.5588235294117645, 'human/per_minute_metrics/n
ERROR - train_mbag - Failed after 0:04:38!
Traceback (most recent calls WITHOUT Sacred internals):
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 1045, in main
    run.log(result)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 435, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1946, in log
    self._log(data=data, step=step, commit=commit)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1659, in _log
    self._partial_history_callback(data, step, commit)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1489, in _partial_history_callback
    self._backend.interface.publish_partial_history(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 684, in publish_partial_history
    item.value_json = json_dumps_safer_history(v)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/util.py", line 816, in json_dumps_safer_history
    return dumps(obj, cls=WandBHistoryJSONEncoder, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/util.py", line 779, in default
    return json.JSONEncoder.default(self, obj)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Tuple is not JSON serializable
[0m

[2m[36m(RolloutWorker pid=3112739)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
