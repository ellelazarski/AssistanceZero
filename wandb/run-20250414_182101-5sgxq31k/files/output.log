INFO - main - Starting training iteration 0
[2m[36m(pid=3116088)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=3116083)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3116083)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3116083)[0m 2025-04-14 18:20:14,646	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.[32m [repeated 19x across cluster][0m
[2m[36m(RolloutWorker pid=3116080)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3116080)[0m   F.linear([32m [repeated 20x across cluster][0m
[2m[36m(RolloutWorker pid=3116083)[0m no player locations found in observation[32m [repeated 57x across cluster][0m
[2m[36m(RolloutWorker pid=3116083)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 19x across cluster][0m
2025-04-14 18:21:15,934	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!
2025-04-14 18:21:16,672	WARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!
/home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  F.linear(
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return F.linear(input, self.weight, self.bias)
/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/autograd/graph.py:823: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2m[36m(pid=3119799)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3116095)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 39x across cluster][0m
[2m[36m(RolloutWorker pid=3116095)[0m   F.linear([32m [repeated 19x across cluster][0m
[2m[36m(RolloutWorker pid=3116095)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 20x across cluster][0m
[2m[36m(RolloutWorker pid=3119799)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3119799)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3119799)[0m 2025-04-14 18:21:19,295	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3119799)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3119799)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3119799)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3119799)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3119799)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:22:10,520 E 3114869 3114869] (raylet) node_manager.cc:3007: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ed8c538fa2a89834cfc1e03f49e9a217ae110df2e3d9005901133c87, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3120323)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3119799)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3119799)[0m   F.linear(
[2m[36m(RolloutWorker pid=3119799)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3120323)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3120323)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3120323)[0m 2025-04-14 18:22:28,675	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3120323)[0m   F.linear(
[2m[36m(RolloutWorker pid=3120323)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120323)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3120323)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120323)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120323)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3120323)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:23:10,520 E 3114869 3114869] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ed8c538fa2a89834cfc1e03f49e9a217ae110df2e3d9005901133c87, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(pid=3120899)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=3120323)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=3120899)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3120899)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=3120899)[0m 2025-04-14 18:23:37,525	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=3120899)[0m   F.linear(
[2m[36m(RolloutWorker pid=3120899)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120899)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=3120899)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120899)[0m no player locations found in observation
[2m[36m(RolloutWorker pid=3120899)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=3120899)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-14 18:24:10,521 E 3114869 3114869] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ed8c538fa2a89834cfc1e03f49e9a217ae110df2e3d9005901133c87, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
ERROR - train_mbag - Failed after 0:04:36!
Traceback (most recent calls WITHOUT Sacred internals):
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 1048, in main
    run.log(result)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 483, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 435, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1946, in log
    self._log(data=data, step=step, commit=commit)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1659, in _log
    self._partial_history_callback(data, step, commit)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1489, in _partial_history_callback
    self._backend.interface.publish_partial_history(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 684, in publish_partial_history
    item.value_json = json_dumps_safer_history(v)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/util.py", line 816, in json_dumps_safer_history
    return dumps(obj, cls=WandBHistoryJSONEncoder, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/wandb/util.py", line 779, in default
    return json.JSONEncoder.default(self, obj)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Tuple is not JSON serializable
[0m

[2m[36m(RolloutWorker pid=3120899)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 2x across cluster][0m
