INFO - main - Starting training iteration 0
[2m[36m(RolloutWorker pid=53028)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=53028)[0m   F.linear(
[2m[36m(RolloutWorker pid=53028)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(pid=53023)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=53028)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=53028)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[36m(RolloutWorker pid=53025)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 23x across cluster][0m
[2m[36m(RolloutWorker pid=53025)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 23x across cluster][0m
[2m[36m(RolloutWorker pid=53026)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 15x across cluster][0m
[2m[36m(RolloutWorker pid=53026)[0m   F.linear([32m [repeated 7x across cluster][0m
[2m[36m(RolloutWorker pid=53026)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 7x across cluster][0m
[2m[36m(pid=55228)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=53025)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 7x across cluster][0m
[2m[36m(RolloutWorker pid=53025)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 7x across cluster][0m
[2m[36m(RolloutWorker pid=55228)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=55228)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(RolloutWorker pid=55228)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=55228)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(pid=55575)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=55575)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=55575)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(pid=56036)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=55609)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=55609)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-19 23:34:38,263 E 51813 51813] (raylet) node_manager.cc:3007: 71 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c1cc018d18d7bdf293d31b789f14bc0329273c130e390d75eb2dd20d, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
2025-04-19 23:35:08,602	WARNING worker.py:2058 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff929d6c9a3978358a77c84a4101000000 Worker ID: 5abdf201f2b51b266fe815275e4c587c9cf237a99fbc479f310a7d47 Node ID: c1cc018d18d7bdf293d31b789f14bc0329273c130e390d75eb2dd20d Worker IP address: 10.51.114.46 Worker port: 45651 Worker PID: 53028 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2m[36m(pid=59255)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=56036)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=56036)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 2x across cluster][0m
[2m[36m(pid=59309)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[33m(raylet)[0m [2025-04-19 23:35:38,263 E 51813 51813] (raylet) node_manager.cc:3007: 45 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c1cc018d18d7bdf293d31b789f14bc0329273c130e390d75eb2dd20d, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(RolloutWorker pid=59255)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 6x across cluster][0m
[2m[36m(RolloutWorker pid=59255)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 6x across cluster][0m
[2m[36m(pid=59651)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 2x across cluster][0m
[2m[36m(RolloutWorker pid=59752)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=59752)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=59797)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[33m(raylet)[0m [2025-04-19 23:39:38,272 E 51813 51813] (raylet) node_manager.cc:3007: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c1cc018d18d7bdf293d31b789f14bc0329273c130e390d75eb2dd20d, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2m[36m(RolloutWorker pid=59905)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=59905)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=59948)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60050)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60050)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=60093)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60191)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60191)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=60234)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60333)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60333)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=60376)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60474)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60474)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 3x across cluster][0m
[2m[36m(pid=60526)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=60526)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=60526)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[36m(pid=60827)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=60827)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m
[2m[36m(RolloutWorker pid=60827)[0m   gym.logger.warn("Casting input x to numpy array.")
[2m[33m(raylet)[0m [2025-04-19 23:40:38,464 E 51813 51813] (raylet) node_manager.cc:3007: 72 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c1cc018d18d7bdf293d31b789f14bc0329273c130e390d75eb2dd20d, IP: 10.51.114.46) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.51.114.46`
[2m[33m(raylet)[0m
[2m[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
WARNING - train_mbag - Aborted after 0:08:13!
Traceback (most recent call last):
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 961, in <module>
    def main(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 190, in automain
    self.run_commandline()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 312, in run_commandline
    return self.run(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 276, in run
    run()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/run.py", line 238, in __call__
    self.result = self.main_function(*args)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/config/captured_function.py", line 42, in captured_function
    result = wrapped(*args, **kwargs)
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 1052, in main
    result = trainer.train()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/tune/trainable/trainable.py", line 397, in train
    result = self.step()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 2838, in _run_one_training_iteration
    results = self.training_step()
  File "/home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero.py", line 538, in training_step
    self._sample_and_add_to_replay_buffer()
  File "/home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero.py", line 472, in _sample_and_add_to_replay_buffer
    new_sample_batches = synchronous_parallel_sample(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py", line 85, in synchronous_parallel_sample
    sample_batches = worker_set.foreach_worker(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 671, in foreach_worker
    remote_results = self.__worker_manager.foreach_actor(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 592, in foreach_actor
    _, remote_results = self.__fetch_result(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 460, in __fetch_result
    ready, _ = ray.wait(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/worker.py", line 2755, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
  File "python/ray/_raylet.pyx", line 3333, in ray._raylet.CoreWorker.wait
  File "python/ray/_raylet.pyx", line 445, in ray._raylet.check_status
KeyboardInterrupt
[0m
