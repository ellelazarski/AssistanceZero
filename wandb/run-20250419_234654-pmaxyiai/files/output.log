INFO - main - Starting training iteration 0
[2m[36m(RolloutWorker pid=65534)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/torch_models.py:84: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)
[2m[36m(RolloutWorker pid=65534)[0m   F.linear(
[2m[36m(RolloutWorker pid=65534)[0m   return F.linear(input, self.weight, self.bias)
[2m[36m(RolloutWorker pid=65538)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log
[2m[36m(RolloutWorker pid=65538)[0m   action_dist_inputs = np.log(mcts_policies)
[2m[36m(pid=65538)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=65536)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/gymnasium/spaces/box.py:230: UserWarning: [33mWARN: Casting input x to numpy array.[0m[32m [repeated 11x across cluster][0m
[2m[36m(RolloutWorker pid=65536)[0m   gym.logger.warn("Casting input x to numpy array.")[32m [repeated 11x across cluster][0m
WARNING - train_mbag - Aborted after 0:10:03!
Traceback (most recent call last):
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 961, in <module>
    def main(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 190, in automain
    self.run_commandline()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 312, in run_commandline
    return self.run(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/experiment.py", line 276, in run
    run()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/run.py", line 238, in __call__
    self.result = self.main_function(*args)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/sacred/config/captured_function.py", line 42, in captured_function
    result = wrapped(*args, **kwargs)
  File "/home/elle/Documents/AssistanceZero/mbag/scripts/train.py", line 1052, in main
    result = trainer.train()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/tune/trainable/trainable.py", line 397, in train
    result = self.step()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 2838, in _run_one_training_iteration
    results = self.training_step()
  File "/home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero.py", line 538, in training_step
    self._sample_and_add_to_replay_buffer()
  File "/home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero.py", line 472, in _sample_and_add_to_replay_buffer
    new_sample_batches = synchronous_parallel_sample(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py", line 85, in synchronous_parallel_sample
    sample_batches = worker_set.foreach_worker(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 671, in foreach_worker
    remote_results = self.__worker_manager.foreach_actor(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 592, in foreach_actor
    _, remote_results = self.__fetch_result(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 460, in __fetch_result
    ready, _ = ray.wait(
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/ray/_private/worker.py", line 2755, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
  File "python/ray/_raylet.pyx", line 3333, in ray._raylet.CoreWorker.wait
  File "python/ray/_raylet.pyx", line 445, in ray._raylet.check_status
KeyboardInterrupt
[0m
[2m[36m(RolloutWorker pid=65539)[0m /home/elle/miniconda3/envs/assistancezero/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:144.)[32m [repeated 7x across cluster][0m
[2m[36m(RolloutWorker pid=65539)[0m   F.linear([32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=65539)[0m   return F.linear(input, self.weight, self.bias)[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=65536)[0m /home/elle/Documents/AssistanceZero/mbag/rllib/alpha_zero/alpha_zero_policy.py:585: RuntimeWarning: divide by zero encountered in log[32m [repeated 3x across cluster][0m
[2m[36m(RolloutWorker pid=65536)[0m   action_dist_inputs = np.log(mcts_policies)[32m [repeated 3x across cluster][0m
